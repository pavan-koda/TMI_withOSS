# PDF Q&A System with OpenAI gpt-oss-20b (CPU+GPU Offload)

A powerful PDF question-answering system using **OpenAI's gpt-oss-20b** (21B parameters, 3.6B active) with CPU+GPU offloading for efficient inference on systems with limited VRAM.

**Model**: OpenAI gpt-oss-20b (Apache 2.0 License)
**Architecture**: Mixture-of-Experts (MoE) - 21B params, 3.6B active
**Quantization**: MXFP4 (fits in 16GB RAM)

---

## üåü Features

### Core Functionality
- ‚úÖ **PDF Upload & Processing** - Upload PDFs up to 50MB, extract text and images
- ‚úÖ **Multi-Page Answers** - Get comprehensive answers citing multiple pages (e.g., "Pages 5, 12-14, 18")
- ‚úÖ **Exact Page References** - Every answer includes specific page numbers where information was found
- ‚úÖ **Confidence Scores** - Each answer includes a 0-100% confidence score
- ‚úÖ **Response Time** - Optimized for 5-15 second responses
- ‚úÖ **Image Support** - Displays diagrams, charts, and images from PDF

### AI & Performance
- ‚úÖ **OpenAI gpt-oss-20b** - 21B param MoE model, 3.6B active (Apache 2.0)
- ‚úÖ **CPU+GPU Offloading** - Offload 15 layers to GPU (4GB VRAM supported)
- ‚úÖ **MXFP4 Quantization** - Efficient memory usage (~16GB RAM)
- ‚úÖ **Smart Context** - Retrieves relevant pages using semantic search
- ‚úÖ **Conversation History** - Remembers last 5 Q&A exchanges for follow-up questions
- ‚úÖ **Reasoning Levels** - Adjustable reasoning effort (low, medium, high)

### Analytics & Logging
- ‚úÖ **Performance Tracking** - Log all questions, answers, response times
- ‚úÖ **Dashboard** - View analytics: total questions, avg response time, avg confidence
- ‚úÖ **Session Management** - Track multiple PDF sessions

---

## üìã System Requirements

### Minimum Requirements
- **RAM**: 16 GB (12GB for model + 4GB for system)
- **GPU**: NVIDIA with 4 GB VRAM (optional but recommended)
- **Storage**: 20 GB free (model + dependencies + data)
- **OS**: Ubuntu 24 or similar Linux (tested on Ubuntu 24.04)

### Recommended Requirements
- **RAM**: 32 GB
- **GPU**: NVIDIA with 6+ GB VRAM
- **Storage**: 50 GB free
- **CPU**: 8+ cores

---

## üöÄ Quick Start

### Option 1: Automated Setup with Ollama (Recommended)

```bash
# Clone or extract project
cd TMI_withOSS

# Run setup script (will install Ollama and gpt-oss-20b)
chmod +x setup.sh
./setup.sh
# Choose option 1 for Ollama installation

# Activate virtual environment
source venv/bin/activate

# Start application
python app_pdf_qa.py
```

### Option 2: Manual Setup with Ollama

```bash
# 1. Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. Download gpt-oss-20b model
ollama pull gpt-oss:20b

# 3. Create virtual environment
python3 -m venv venv
source venv/bin/activate

# 4. Install llama-cpp-python with CUDA support (for GPU)
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python==0.2.27 --force-reinstall --no-cache-dir

# 5. Install other dependencies
pip install -r requirements.txt

# 6. Find Ollama model location and create symlink
ls ~/.ollama/models/blobs/sha256-*
ln -s ~/.ollama/models/blobs/sha256-xxxxx models/gpt-oss-20b.gguf

# 7. Update model_config.py (or use symlink path)
nano model_config.py
# Set: "model_path": "models/gpt-oss-20b.gguf"

# 8. Start application
python app_pdf_qa.py
```

**Detailed Installation Guide**: See [INSTALL_GPT_OSS.md](INSTALL_GPT_OSS.md)

### Access the Application

Open browser and visit: **http://localhost:5000**

---

## üì¶ Project Structure

```
TMI_withOSS/
‚îú‚îÄ‚îÄ app_pdf_qa.py              # Main Flask application
‚îú‚îÄ‚îÄ pdf_qa_engine.py           # QA engine with 20B LLM
‚îú‚îÄ‚îÄ pdf_processor.py           # PDF processing (text + images)
‚îú‚îÄ‚îÄ model_config.py            # Model configuration (GPU layers, paths, etc.)
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ setup.sh                   # Automated setup script
‚îÇ
‚îú‚îÄ‚îÄ templates/                 # HTML templates
‚îÇ   ‚îú‚îÄ‚îÄ upload.html            # PDF upload page
‚îÇ   ‚îî‚îÄ‚îÄ qa.html                # Q&A chat interface
‚îÇ
‚îú‚îÄ‚îÄ static/                    # CSS, JavaScript, images
‚îÇ
‚îú‚îÄ‚îÄ models/                    # GGUF model files (you download these)
‚îÇ   ‚îî‚îÄ‚îÄ [your-model].gguf
‚îÇ
‚îú‚îÄ‚îÄ uploads/                   # Temporary PDF uploads
‚îú‚îÄ‚îÄ processed_pdfs/            # Extracted page images
‚îú‚îÄ‚îÄ chroma_db/                 # Vector database
‚îú‚îÄ‚îÄ logs/                      # Performance logs
‚îî‚îÄ‚îÄ data/                      # Session data

```

---

## ‚öôÔ∏è Configuration

### GPU Offloading (model_config.py)

Adjust `n_gpu_layers` based on your GPU VRAM:

```python
MODEL_CONFIG = {
    # GPU offloading
    "n_gpu_layers": 12,  # Recommended for 4GB VRAM

    # For different VRAM:
    # - 0: CPU only (no GPU)
    # - 10-15: 4GB VRAM
    # - 20-25: 6GB VRAM
    # - 35+: 8GB+ VRAM
}
```

### Model Selection

Edit `model_config.py` to change the model:

```python
MODEL_CONFIG = {
    "model_path": "models/your-model.gguf",
}
```

**Recommended Model: OpenAI gpt-oss-20b** ‚≠ê

| Model | Params | Active | RAM | License | Quality |
|-------|--------|--------|-----|---------|---------|
| **gpt-oss-20b** | 21B | 3.6B | 16GB | Apache 2.0 | **Excellent** |

**Installation**:
```bash
# Easiest method - using Ollama
ollama pull gpt-oss:20b
```

**Why gpt-oss-20b?**
- ‚úÖ Perfect for your hardware (39GB RAM + 4GB VRAM)
- ‚úÖ Mixture-of-Experts: Only 3.6B active params per token
- ‚úÖ MXFP4 quantization: Fits in 16GB RAM
- ‚úÖ Apache 2.0 license: Fully permissive
- ‚úÖ Built-in reasoning levels and function calling

Download: https://huggingface.co/openai/gpt-oss-20b
Ollama: https://ollama.com/library/gpt-oss

---

## üìä How It Works

### 1. PDF Processing
```
Upload PDF ‚Üí Extract text from each page
          ‚Üí Render each page as image (150 DPI)
          ‚Üí Extract embedded images
```

### 2. Indexing
```
Text Index (ChromaDB)
‚îú‚îÄ Full text from each page
‚îú‚îÄ Semantic embeddings
‚îî‚îÄ Fast retrieval by similarity

File System
‚îî‚îÄ Page images for display
```

### 3. Question Answering
```
Your Question
    ‚Üì
Semantic Search ‚Üí Find top 5 relevant pages
    ‚Üì
Build Context ‚Üí Combine text from relevant pages
    ‚Üì
20B LLM Inference ‚Üí Generate comprehensive answer
    ‚Üì
Calculate Confidence ‚Üí 0-100% based on retrieval + quality
    ‚Üì
Format Response ‚Üí Include answer, pages, confidence, time
```

### 4. Multi-Page Citations

Example answer format:

```
Revenue increased by 25% due to Asia expansion (Page 5) and
new product launches (Pages 12-14). The detailed strategy is
outlined on Page 18.

üìÑ Pages Referenced: Pages 5, 12-14, 18
üìä Confidence: 87.3%
‚è±Ô∏è Response Time: 8.2s
```

---

## üéØ Features in Detail

### 1. Multi-Page Answer Support

The system automatically:
- Retrieves multiple relevant pages (top 5 by default)
- Combines text context from all pages
- Cites exact pages in the answer
- Groups consecutive pages into ranges (e.g., "12-14")

### 2. Confidence Scoring

Confidence score (0-100%) is calculated based on:
- **Retrieval Confidence (40%)**: How relevant the retrieved pages are
- **Answer Quality (40%)**: Length and completeness of answer
- **Context Usage (20%)**: How well the answer uses the source text

### 3. Performance Optimization

- **GPU Offloading**: Offload transformer layers to GPU for 3-5x speedup
- **GGUF Quantization**: 4-bit quantization reduces model size by 75%
- **Smart Context**: Only retrieves relevant pages (not entire PDF)
- **Streaming Disabled**: Faster single-shot generation

### 4. Analytics Dashboard

Visit **http://localhost:5000/view-log** to see:
- Total questions asked
- Average response time
- Average confidence score
- Recent activity log
- Detailed per-question metrics

---

## üìà Expected Performance

### With GPU Offloading (12 layers on 4GB VRAM)

| Task | Time | Throughput |
|------|------|------------|
| PDF Processing (100 pages) | 30-60s | ~2 pages/sec |
| Index Creation | 5-10s | - |
| Question Answering | 5-15s | ~20-30 tokens/sec |

### CPU Only (No GPU)

| Task | Time | Throughput |
|------|------|------------|
| PDF Processing (100 pages) | 30-60s | ~2 pages/sec |
| Index Creation | 5-10s | - |
| Question Answering | 30-90s | ~5-10 tokens/sec |

---

## üîß Troubleshooting

### Issue: llama-cpp-python fails to install

**Solution**: Install build dependencies first

```bash
sudo apt-get update
sudo apt-get install build-essential cmake
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
```

### Issue: Model file not found

**Solution**: Download model and update config

```bash
# Download from HuggingFace
wget https://huggingface.co/TheBloke/Qwen2.5-14B-Instruct-GGUF/resolve/main/qwen2.5-14b-instruct.Q4_K_M.gguf -O models/qwen2.5-14b-instruct.Q4_K_M.gguf

# Update config
nano model_config.py
# Set: "model_path": "models/qwen2.5-14b-instruct.Q4_K_M.gguf"
```

### Issue: CUDA out of memory

**Solution**: Reduce GPU layers

```python
# In model_config.py
MODEL_CONFIG = {
    "n_gpu_layers": 8,  # Reduce from 12 to 8
}
```

### Issue: Slow inference (CPU only)

**Solution**: Install with CUDA support

```bash
CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python --force-reinstall --no-cache-dir
```

Then edit `model_config.py`:

```python
"n_gpu_layers": 12,  # Offload layers to GPU
```

### Issue: Port 5000 already in use

**Solution**: Change port in `app_pdf_qa.py`

```python
# Line 525
app.run(host='0.0.0.0', port=5001, debug=False)
```

---

## üìù API Endpoints

### Upload PDF
```
POST /upload
Body: multipart/form-data
  - pdf_file: PDF file
  - dpi: 100, 150, 200, or 300 (default: 150)
  - extract_images: true or false (default: true)

Response:
{
  "success": true,
  "message": "PDF processed successfully! 45 pages indexed.",
  "metadata": {
    "filename": "document.pdf",
    "total_pages": 45,
    "session_id": "abc-123"
  }
}
```

### Ask Question
```
POST /ask
Body: application/json
{
  "question": "What is the revenue growth?",
  "top_k": 5
}

Response:
{
  "success": true,
  "answer": "Revenue grew by 25%...",
  "page_references": "Pages 5, 12-14, 18",
  "confidence": 87.3,
  "response_time": 8.2,
  "pages_used": [5, 12, 13, 14, 18],
  "images": ["/data/session-id/page_0005.png", ...]
}
```

### Analytics
```
GET /analytics

Response:
{
  "total_questions": 42,
  "avg_response_time": 9.5,
  "avg_confidence": 85.2,
  "recent_logs": [...]
}
```

### Health Check
```
GET /health

Response:
{
  "status": "healthy",
  "qa_engine": true,
  "model_info": {
    "model_path": "models/qwen2.5-14b-instruct.Q4_K_M.gguf",
    "n_gpu_layers": 12,
    "n_ctx": 4096
  }
}
```

---

## üõ°Ô∏è Security Notes

- Runs on localhost by default (not exposed to network)
- All processing is local (no external API calls)
- Session data cleared on restart
- Uploaded PDFs stored temporarily

To enable external access (use with caution):
```python
# app_pdf_qa.py line 525
app.run(host='0.0.0.0', port=5000, debug=False)
# Then access from network: http://<server-ip>:5000
```

---

## üìö Model: OpenAI gpt-oss-20b

### Perfect for Your System (39GB RAM, 4GB VRAM)

**OpenAI gpt-oss-20b** (Apache 2.0 License)
- **Parameters**: 21B total, 3.6B active (MoE)
- **Memory**: ~16GB RAM with MXFP4 quantization
- **Quality**: Excellent for document Q&A and reasoning
- **Speed**: 20-30 tokens/sec with 15 GPU layers
- **License**: Apache 2.0 (fully permissive)

**Key Features**:
- ‚úÖ Mixture-of-Experts (MoE) architecture
- ‚úÖ MXFP4 quantization for efficient memory usage
- ‚úÖ Configurable reasoning levels (low/medium/high)
- ‚úÖ Full chain-of-thought access
- ‚úÖ Built-in function calling and tool use
- ‚úÖ Native web browsing capabilities

**Installation**:
```bash
# Using Ollama (easiest)
ollama pull gpt-oss:20b

# Using HuggingFace
huggingface-cli download openai/gpt-oss-20b --include "original/*" --local-dir models/gpt-oss-20b/
```

**Resources**:
- Model Card: https://huggingface.co/openai/gpt-oss-20b
- Ollama: https://ollama.com/library/gpt-oss
- GitHub: https://github.com/openai/gpt-oss
- Installation Guide: [INSTALL_GPT_OSS.md](INSTALL_GPT_OSS.md)

---

## üìñ Usage Examples

### Example 1: Financial Document

```
Q: What was the revenue growth in Q4 2023?
A: Revenue grew by 25% in Q4 2023, reaching $15.2 million. This growth
   was driven by expansion into Asian markets (Page 5) and the launch of
   three new product lines (Pages 12-14). The detailed breakdown of revenue
   by region can be found on Page 18.

üìÑ Pages: 5, 12-14, 18
üìä Confidence: 92.1%
‚è±Ô∏è Time: 7.3s
```

### Example 2: Technical Manual

```
Q: How do I configure the firewall settings?
A: To configure the firewall, navigate to Settings > Network > Firewall
   (Page 34). Enable the firewall by toggling the switch, then add rules
   using the "Add Rule" button (Page 35). For advanced configurations
   including port forwarding and DMZ setup, refer to Pages 38-40.

üìÑ Pages: 34-35, 38-40
üìä Confidence: 88.5%
‚è±Ô∏è Time: 9.1s
```

---

## ü§ù Contributing

This project is maintained internally. For issues or feature requests, please contact the development team.

---

## üìú License

Internal use only. Not for redistribution.

---

## üôè Acknowledgments

- **llama.cpp** - Efficient LLM inference
- **ChromaDB** - Vector database
- **PyMuPDF** - PDF processing
- **Flask** - Web framework
- **TheBloke** - GGUF model quantization

---

## üìû Support

For questions or issues:
1. Check logs in `logs/qa_performance.txt`
2. Check Flask output in terminal
3. Verify model exists: `python model_config.py`
4. Test health endpoint: `curl http://localhost:5000/health`

---

**Ready to process PDFs with AI!** üöÄ

Upload a PDF, ask questions, and get comprehensive answers with exact page references and confidence scores.
